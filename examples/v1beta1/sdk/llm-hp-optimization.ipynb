{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Explore Kubeflow Katib Hyperparamter Optimization API",
   "id": "58ad086d543ba3a1"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-01-16T13:42:54.643224Z",
     "start_time": "2025-01-16T13:42:46.916516Z"
    }
   },
   "source": [
    "!pip install git+https://github.com/kubeflow/katib.git#subdirectory=sdk/python/v1beta1\n",
    "!pip install git+https://github.com/kubeflow/training-operator.git#subdirectory=sdk/python"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/kubeflow/katib.git#subdirectory=sdk/python/v1beta1\r\n",
      "  Cloning https://github.com/kubeflow/katib.git to /private/var/folders/_z/49hgcx6x4db7pjpfzjcphlcm0000gn/T/pip-req-build-a6ajaxks\r\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/kubeflow/katib.git /private/var/folders/_z/49hgcx6x4db7pjpfzjcphlcm0000gn/T/pip-req-build-a6ajaxks\r\n",
      "  Resolved https://github.com/kubeflow/katib.git to commit 93bee4dc25529a92320cf33634bb6920013c8281\r\n",
      "  Preparing metadata (setup.py) ... \u001B[?25ldone\r\n",
      "\u001B[?25hRequirement already satisfied: certifi>=14.05.14 in /Users/mahdikhashan/miniconda3/envs/llm-hp-optimization-katib-nb/lib/python3.9/site-packages (from kubeflow-katib==0.17.0) (2024.12.14)\r\n",
      "Requirement already satisfied: six>=1.10 in /Users/mahdikhashan/miniconda3/envs/llm-hp-optimization-katib-nb/lib/python3.9/site-packages (from kubeflow-katib==0.17.0) (1.16.0)\r\n",
      "Requirement already satisfied: setuptools>=21.0.0 in /Users/mahdikhashan/miniconda3/envs/llm-hp-optimization-katib-nb/lib/python3.9/site-packages (from kubeflow-katib==0.17.0) (75.1.0)\r\n",
      "Requirement already satisfied: urllib3>=1.15.1 in /Users/mahdikhashan/miniconda3/envs/llm-hp-optimization-katib-nb/lib/python3.9/site-packages (from kubeflow-katib==0.17.0) (2.2.3)\r\n",
      "Requirement already satisfied: kubernetes>=27.2.0 in /Users/mahdikhashan/miniconda3/envs/llm-hp-optimization-katib-nb/lib/python3.9/site-packages (from kubeflow-katib==0.17.0) (31.0.0)\r\n",
      "Requirement already satisfied: grpcio>=1.64.1 in /Users/mahdikhashan/miniconda3/envs/llm-hp-optimization-katib-nb/lib/python3.9/site-packages (from kubeflow-katib==0.17.0) (1.69.0)\r\n",
      "Requirement already satisfied: protobuf<5,>=4.21.12 in /Users/mahdikhashan/miniconda3/envs/llm-hp-optimization-katib-nb/lib/python3.9/site-packages (from kubeflow-katib==0.17.0) (4.25.5)\r\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in /Users/mahdikhashan/miniconda3/envs/llm-hp-optimization-katib-nb/lib/python3.9/site-packages (from kubernetes>=27.2.0->kubeflow-katib==0.17.0) (2.9.0.post0)\r\n",
      "Requirement already satisfied: pyyaml>=5.4.1 in /Users/mahdikhashan/miniconda3/envs/llm-hp-optimization-katib-nb/lib/python3.9/site-packages (from kubernetes>=27.2.0->kubeflow-katib==0.17.0) (6.0.2)\r\n",
      "Requirement already satisfied: google-auth>=1.0.1 in /Users/mahdikhashan/miniconda3/envs/llm-hp-optimization-katib-nb/lib/python3.9/site-packages (from kubernetes>=27.2.0->kubeflow-katib==0.17.0) (2.37.0)\r\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /Users/mahdikhashan/miniconda3/envs/llm-hp-optimization-katib-nb/lib/python3.9/site-packages (from kubernetes>=27.2.0->kubeflow-katib==0.17.0) (1.8.0)\r\n",
      "Requirement already satisfied: requests in /Users/mahdikhashan/miniconda3/envs/llm-hp-optimization-katib-nb/lib/python3.9/site-packages (from kubernetes>=27.2.0->kubeflow-katib==0.17.0) (2.32.3)\r\n",
      "Requirement already satisfied: requests-oauthlib in /Users/mahdikhashan/miniconda3/envs/llm-hp-optimization-katib-nb/lib/python3.9/site-packages (from kubernetes>=27.2.0->kubeflow-katib==0.17.0) (2.0.0)\r\n",
      "Requirement already satisfied: oauthlib>=3.2.2 in /Users/mahdikhashan/miniconda3/envs/llm-hp-optimization-katib-nb/lib/python3.9/site-packages (from kubernetes>=27.2.0->kubeflow-katib==0.17.0) (3.2.2)\r\n",
      "Requirement already satisfied: durationpy>=0.7 in /Users/mahdikhashan/miniconda3/envs/llm-hp-optimization-katib-nb/lib/python3.9/site-packages (from kubernetes>=27.2.0->kubeflow-katib==0.17.0) (0.9)\r\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /Users/mahdikhashan/miniconda3/envs/llm-hp-optimization-katib-nb/lib/python3.9/site-packages (from google-auth>=1.0.1->kubernetes>=27.2.0->kubeflow-katib==0.17.0) (5.5.0)\r\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /Users/mahdikhashan/miniconda3/envs/llm-hp-optimization-katib-nb/lib/python3.9/site-packages (from google-auth>=1.0.1->kubernetes>=27.2.0->kubeflow-katib==0.17.0) (0.4.1)\r\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /Users/mahdikhashan/miniconda3/envs/llm-hp-optimization-katib-nb/lib/python3.9/site-packages (from google-auth>=1.0.1->kubernetes>=27.2.0->kubeflow-katib==0.17.0) (4.9)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/mahdikhashan/miniconda3/envs/llm-hp-optimization-katib-nb/lib/python3.9/site-packages (from requests->kubernetes>=27.2.0->kubeflow-katib==0.17.0) (3.3.2)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/mahdikhashan/miniconda3/envs/llm-hp-optimization-katib-nb/lib/python3.9/site-packages (from requests->kubernetes>=27.2.0->kubeflow-katib==0.17.0) (3.7)\r\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /Users/mahdikhashan/miniconda3/envs/llm-hp-optimization-katib-nb/lib/python3.9/site-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=27.2.0->kubeflow-katib==0.17.0) (0.6.1)\r\n",
      "Collecting git+https://github.com/kubeflow/training-operator.git#subdirectory=sdk/python\r\n",
      "  Cloning https://github.com/kubeflow/training-operator.git to /private/var/folders/_z/49hgcx6x4db7pjpfzjcphlcm0000gn/T/pip-req-build-95zfygeb\r\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/kubeflow/training-operator.git /private/var/folders/_z/49hgcx6x4db7pjpfzjcphlcm0000gn/T/pip-req-build-95zfygeb\r\n",
      "  Resolved https://github.com/kubeflow/training-operator.git to commit 1dfa40c12516fc9eb2ce12c5ef52da7d46670457\r\n",
      "  Preparing metadata (setup.py) ... \u001B[?25ldone\r\n",
      "\u001B[?25hRequirement already satisfied: certifi>=14.05.14 in /Users/mahdikhashan/miniconda3/envs/llm-hp-optimization-katib-nb/lib/python3.9/site-packages (from kubeflow-training==1.8.1) (2024.12.14)\r\n",
      "Requirement already satisfied: six>=1.10 in /Users/mahdikhashan/miniconda3/envs/llm-hp-optimization-katib-nb/lib/python3.9/site-packages (from kubeflow-training==1.8.1) (1.16.0)\r\n",
      "Requirement already satisfied: setuptools>=21.0.0 in /Users/mahdikhashan/miniconda3/envs/llm-hp-optimization-katib-nb/lib/python3.9/site-packages (from kubeflow-training==1.8.1) (75.1.0)\r\n",
      "Requirement already satisfied: urllib3>=1.15.1 in /Users/mahdikhashan/miniconda3/envs/llm-hp-optimization-katib-nb/lib/python3.9/site-packages (from kubeflow-training==1.8.1) (2.2.3)\r\n",
      "Requirement already satisfied: kubernetes>=27.2.0 in /Users/mahdikhashan/miniconda3/envs/llm-hp-optimization-katib-nb/lib/python3.9/site-packages (from kubeflow-training==1.8.1) (31.0.0)\r\n",
      "Collecting retrying>=1.3.3 (from kubeflow-training==1.8.1)\r\n",
      "  Downloading retrying-1.3.4-py3-none-any.whl.metadata (6.9 kB)\r\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in /Users/mahdikhashan/miniconda3/envs/llm-hp-optimization-katib-nb/lib/python3.9/site-packages (from kubernetes>=27.2.0->kubeflow-training==1.8.1) (2.9.0.post0)\r\n",
      "Requirement already satisfied: pyyaml>=5.4.1 in /Users/mahdikhashan/miniconda3/envs/llm-hp-optimization-katib-nb/lib/python3.9/site-packages (from kubernetes>=27.2.0->kubeflow-training==1.8.1) (6.0.2)\r\n",
      "Requirement already satisfied: google-auth>=1.0.1 in /Users/mahdikhashan/miniconda3/envs/llm-hp-optimization-katib-nb/lib/python3.9/site-packages (from kubernetes>=27.2.0->kubeflow-training==1.8.1) (2.37.0)\r\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /Users/mahdikhashan/miniconda3/envs/llm-hp-optimization-katib-nb/lib/python3.9/site-packages (from kubernetes>=27.2.0->kubeflow-training==1.8.1) (1.8.0)\r\n",
      "Requirement already satisfied: requests in /Users/mahdikhashan/miniconda3/envs/llm-hp-optimization-katib-nb/lib/python3.9/site-packages (from kubernetes>=27.2.0->kubeflow-training==1.8.1) (2.32.3)\r\n",
      "Requirement already satisfied: requests-oauthlib in /Users/mahdikhashan/miniconda3/envs/llm-hp-optimization-katib-nb/lib/python3.9/site-packages (from kubernetes>=27.2.0->kubeflow-training==1.8.1) (2.0.0)\r\n",
      "Requirement already satisfied: oauthlib>=3.2.2 in /Users/mahdikhashan/miniconda3/envs/llm-hp-optimization-katib-nb/lib/python3.9/site-packages (from kubernetes>=27.2.0->kubeflow-training==1.8.1) (3.2.2)\r\n",
      "Requirement already satisfied: durationpy>=0.7 in /Users/mahdikhashan/miniconda3/envs/llm-hp-optimization-katib-nb/lib/python3.9/site-packages (from kubernetes>=27.2.0->kubeflow-training==1.8.1) (0.9)\r\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /Users/mahdikhashan/miniconda3/envs/llm-hp-optimization-katib-nb/lib/python3.9/site-packages (from google-auth>=1.0.1->kubernetes>=27.2.0->kubeflow-training==1.8.1) (5.5.0)\r\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /Users/mahdikhashan/miniconda3/envs/llm-hp-optimization-katib-nb/lib/python3.9/site-packages (from google-auth>=1.0.1->kubernetes>=27.2.0->kubeflow-training==1.8.1) (0.4.1)\r\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /Users/mahdikhashan/miniconda3/envs/llm-hp-optimization-katib-nb/lib/python3.9/site-packages (from google-auth>=1.0.1->kubernetes>=27.2.0->kubeflow-training==1.8.1) (4.9)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/mahdikhashan/miniconda3/envs/llm-hp-optimization-katib-nb/lib/python3.9/site-packages (from requests->kubernetes>=27.2.0->kubeflow-training==1.8.1) (3.3.2)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/mahdikhashan/miniconda3/envs/llm-hp-optimization-katib-nb/lib/python3.9/site-packages (from requests->kubernetes>=27.2.0->kubeflow-training==1.8.1) (3.7)\r\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /Users/mahdikhashan/miniconda3/envs/llm-hp-optimization-katib-nb/lib/python3.9/site-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=27.2.0->kubeflow-training==1.8.1) (0.6.1)\r\n",
      "Downloading retrying-1.3.4-py3-none-any.whl (11 kB)\r\n",
      "Building wheels for collected packages: kubeflow-training\r\n",
      "  Building wheel for kubeflow-training (setup.py) ... \u001B[?25ldone\r\n",
      "\u001B[?25h  Created wheel for kubeflow-training: filename=kubeflow_training-1.8.1-py3-none-any.whl size=111180 sha256=3c432a3ba850bb70ff4d17bce13c2882dd0ef9370d3a0ccc083df1431c4bb8f7\r\n",
      "  Stored in directory: /private/var/folders/_z/49hgcx6x4db7pjpfzjcphlcm0000gn/T/pip-ephem-wheel-cache-d1jpqseq/wheels/4e/97/bb/7c46e489ad7772669c94e462b1f545c475d32d70259ba08209\r\n",
      "Successfully built kubeflow-training\r\n",
      "Installing collected packages: retrying, kubeflow-training\r\n",
      "Successfully installed kubeflow-training-1.8.1 retrying-1.3.4\r\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-16T13:32:17.632058Z",
     "start_time": "2025-01-16T13:31:54.022957Z"
    }
   },
   "cell_type": "code",
   "source": "!pip install transformers peft",
   "id": "1cd9f378653acfd0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\r\n",
      "  Downloading transformers-4.48.0-py3-none-any.whl.metadata (44 kB)\r\n",
      "Collecting peft\r\n",
      "  Downloading peft-0.14.0-py3-none-any.whl.metadata (13 kB)\r\n",
      "Collecting filelock (from transformers)\r\n",
      "  Using cached filelock-3.16.1-py3-none-any.whl.metadata (2.9 kB)\r\n",
      "Collecting huggingface-hub<1.0,>=0.24.0 (from transformers)\r\n",
      "  Using cached huggingface_hub-0.27.1-py3-none-any.whl.metadata (13 kB)\r\n",
      "Collecting numpy>=1.17 (from transformers)\r\n",
      "  Downloading numpy-2.0.2-cp39-cp39-macosx_14_0_arm64.whl.metadata (60 kB)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/mahdikhashan/miniconda3/envs/llm-hp-optimization-katib-nb/lib/python3.9/site-packages (from transformers) (24.2)\r\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/mahdikhashan/miniconda3/envs/llm-hp-optimization-katib-nb/lib/python3.9/site-packages (from transformers) (6.0.2)\r\n",
      "Collecting regex!=2019.12.17 (from transformers)\r\n",
      "  Downloading regex-2024.11.6-cp39-cp39-macosx_11_0_arm64.whl.metadata (40 kB)\r\n",
      "Requirement already satisfied: requests in /Users/mahdikhashan/miniconda3/envs/llm-hp-optimization-katib-nb/lib/python3.9/site-packages (from transformers) (2.32.3)\r\n",
      "Collecting tokenizers<0.22,>=0.21 (from transformers)\r\n",
      "  Using cached tokenizers-0.21.0-cp39-abi3-macosx_11_0_arm64.whl.metadata (6.7 kB)\r\n",
      "Collecting safetensors>=0.4.1 (from transformers)\r\n",
      "  Using cached safetensors-0.5.2-cp38-abi3-macosx_11_0_arm64.whl.metadata (3.8 kB)\r\n",
      "Collecting tqdm>=4.27 (from transformers)\r\n",
      "  Using cached tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\r\n",
      "Requirement already satisfied: psutil in /Users/mahdikhashan/miniconda3/envs/llm-hp-optimization-katib-nb/lib/python3.9/site-packages (from peft) (5.9.0)\r\n",
      "Collecting torch>=1.13.0 (from peft)\r\n",
      "  Downloading torch-2.5.1-cp39-none-macosx_11_0_arm64.whl.metadata (28 kB)\r\n",
      "Collecting accelerate>=0.21.0 (from peft)\r\n",
      "  Downloading accelerate-1.2.1-py3-none-any.whl.metadata (19 kB)\r\n",
      "Collecting fsspec>=2023.5.0 (from huggingface-hub<1.0,>=0.24.0->transformers)\r\n",
      "  Using cached fsspec-2024.12.0-py3-none-any.whl.metadata (11 kB)\r\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/mahdikhashan/miniconda3/envs/llm-hp-optimization-katib-nb/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (4.12.2)\r\n",
      "Collecting networkx (from torch>=1.13.0->peft)\r\n",
      "  Downloading networkx-3.2.1-py3-none-any.whl.metadata (5.2 kB)\r\n",
      "Requirement already satisfied: jinja2 in /Users/mahdikhashan/miniconda3/envs/llm-hp-optimization-katib-nb/lib/python3.9/site-packages (from torch>=1.13.0->peft) (3.1.4)\r\n",
      "Collecting sympy==1.13.1 (from torch>=1.13.0->peft)\r\n",
      "  Using cached sympy-1.13.1-py3-none-any.whl.metadata (12 kB)\r\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy==1.13.1->torch>=1.13.0->peft)\r\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/mahdikhashan/miniconda3/envs/llm-hp-optimization-katib-nb/lib/python3.9/site-packages (from requests->transformers) (3.3.2)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/mahdikhashan/miniconda3/envs/llm-hp-optimization-katib-nb/lib/python3.9/site-packages (from requests->transformers) (3.7)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/mahdikhashan/miniconda3/envs/llm-hp-optimization-katib-nb/lib/python3.9/site-packages (from requests->transformers) (2.2.3)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/mahdikhashan/miniconda3/envs/llm-hp-optimization-katib-nb/lib/python3.9/site-packages (from requests->transformers) (2024.12.14)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/mahdikhashan/miniconda3/envs/llm-hp-optimization-katib-nb/lib/python3.9/site-packages (from jinja2->torch>=1.13.0->peft) (2.1.3)\r\n",
      "Downloading transformers-4.48.0-py3-none-any.whl (9.7 MB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m9.7/9.7 MB\u001B[0m \u001B[31m20.3 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0ma \u001B[36m0:00:01\u001B[0m\r\n",
      "\u001B[?25hDownloading peft-0.14.0-py3-none-any.whl (374 kB)\r\n",
      "Downloading accelerate-1.2.1-py3-none-any.whl (336 kB)\r\n",
      "Using cached huggingface_hub-0.27.1-py3-none-any.whl (450 kB)\r\n",
      "Downloading numpy-2.0.2-cp39-cp39-macosx_14_0_arm64.whl (5.3 MB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m5.3/5.3 MB\u001B[0m \u001B[31m12.7 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m00:01\u001B[0m\r\n",
      "\u001B[?25hDownloading regex-2024.11.6-cp39-cp39-macosx_11_0_arm64.whl (284 kB)\r\n",
      "Using cached safetensors-0.5.2-cp38-abi3-macosx_11_0_arm64.whl (408 kB)\r\n",
      "Using cached tokenizers-0.21.0-cp39-abi3-macosx_11_0_arm64.whl (2.6 MB)\r\n",
      "Downloading torch-2.5.1-cp39-none-macosx_11_0_arm64.whl (63.9 MB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m63.9/63.9 MB\u001B[0m \u001B[31m17.4 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m00:01\u001B[0m\r\n",
      "\u001B[?25hUsing cached sympy-1.13.1-py3-none-any.whl (6.2 MB)\r\n",
      "Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)\r\n",
      "Using cached filelock-3.16.1-py3-none-any.whl (16 kB)\r\n",
      "Using cached fsspec-2024.12.0-py3-none-any.whl (183 kB)\r\n",
      "Downloading networkx-3.2.1-py3-none-any.whl (1.6 MB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m1.6/1.6 MB\u001B[0m \u001B[31m22.0 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25hUsing cached mpmath-1.3.0-py3-none-any.whl (536 kB)\r\n",
      "Installing collected packages: mpmath, tqdm, sympy, safetensors, regex, numpy, networkx, fsspec, filelock, torch, huggingface-hub, tokenizers, accelerate, transformers, peft\r\n",
      "Successfully installed accelerate-1.2.1 filelock-3.16.1 fsspec-2024.12.0 huggingface-hub-0.27.1 mpmath-1.3.0 networkx-3.2.1 numpy-2.0.2 peft-0.14.0 regex-2024.11.6 safetensors-0.5.2 sympy-1.13.1 tokenizers-0.21.0 torch-2.5.1 tqdm-4.67.1 transformers-4.48.0\r\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-16T14:24:28.811618Z",
     "start_time": "2025-01-16T14:24:28.787993Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# from kubeflow.katib.types.trainer_resources import TrainerResources\n",
    "# \n",
    "# \n",
    "# resources_per_trial = TrainerResources(\n",
    "#     num_workers=4,                    # Number of distributed workers\n",
    "#     num_procs_per_worker=2,           # Processes per worker\n",
    "#     resources_per_worker={            # Resource allocation per worker\n",
    "#         \"gpu\": 2,                     # Number of GPUs\n",
    "#         \"cpu\": 5,                     # Number of CPUs\n",
    "#         \"memory\": \"10G\",              # Memory allocation\n",
    "#     },\n",
    "# )\n"
   ],
   "id": "f785c7083c5c90ba",
   "outputs": [],
   "execution_count": 51
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-16T14:24:29.166726Z",
     "start_time": "2025-01-16T14:24:29.154174Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import kubeflow.katib as katib\n",
    "from kubeflow.katib import KatibClient\n",
    "\n",
    "from transformers import AutoModelForSequenceClassification, TrainingArguments\n",
    "from peft import LoraConfig\n",
    "\n",
    "from kubeflow.storage_initializer.hugging_face import (\n",
    "    HuggingFaceModelParams,\n",
    "    HuggingFaceDatasetParams,\n",
    "    HuggingFaceTrainerParams,\n",
    ")"
   ],
   "id": "1446506988a7b011",
   "outputs": [],
   "execution_count": 52
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-16T14:28:55.879588Z",
     "start_time": "2025-01-16T14:28:55.761736Z"
    }
   },
   "cell_type": "code",
   "source": [
    "hf_model = HuggingFaceModelParams(\n",
    "    model_uri = \"hf://meta-llama/Llama-3.2-1B\",\n",
    "    transformer_type = AutoModelForSequenceClassification,\n",
    ")\n",
    "\n",
    "# Train the model on 1000 movie reviews from imdb\n",
    "# https://huggingface.co/datasets/stanfordnlp/imdb\n",
    "hf_dataset = HuggingFaceDatasetParams(\n",
    "    repo_id = \"imdb\",\n",
    "    split = \"train[:1000]\",\n",
    ")\n",
    "\n",
    "hf_tuning_parameters = HuggingFaceTrainerParams(\n",
    "    training_parameters = TrainingArguments(\n",
    "        output_dir = \"results\",\n",
    "        save_strategy = \"epoch\",\n",
    "        hub_strategy=\"all_checkpoints\",\n",
    "        learning_rate = 1e-05, #katib.search.double(min=1e-05, max=5e-05),\n",
    "        num_train_epochs=3,\n",
    "    ),\n",
    "    # Set LoRA config to reduce number of trainable model parameters.\n",
    "    lora_config = LoraConfig(\n",
    "        r = 1, # katib.search.int(min=8, max=32),\n",
    "        lora_alpha = 8,\n",
    "        lora_dropout = 0.1,\n",
    "        bias = \"none\",\n",
    "    ),\n",
    ")"
   ],
   "id": "45c5a2476e1bffb7",
   "outputs": [],
   "execution_count": 56
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-16T14:28:56.432645Z",
     "start_time": "2025-01-16T14:28:56.364494Z"
    }
   },
   "cell_type": "code",
   "source": "cl = KatibClient(namespace=\"kubeflow\")",
   "id": "c7995d6934399e6c",
   "outputs": [],
   "execution_count": 57
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-16T14:28:57.141850Z",
     "start_time": "2025-01-16T14:28:56.670285Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Fine-tuning for Binary Classification\n",
    "exp_name = \"testllm\"\n",
    "cl.tune(\n",
    "    name = exp_name,\n",
    "    model_provider_parameters = hf_model,\n",
    "    dataset_provider_parameters = hf_dataset,\n",
    "    trainer_parameters = hf_tuning_parameters,\n",
    "    objective_metric_name = \"train_loss\",\n",
    "    objective_type = \"minimize\",\n",
    "    algorithm_name = \"random\",\n",
    "    # max_trial_count = 10,\n",
    "    # parallel_trial_count = 2,\n",
    "    # resources_per_trial={\n",
    "    #     \"gpu\": \"2\",\n",
    "    #     \"cpu\": \"4\",\n",
    "    #     \"memory\": \"10G\",\n",
    "    # },\n",
    ")\n",
    "\n",
    "cl.wait_for_experiment_condition(name=exp_name)\n",
    "\n",
    "# Get the best hyperparameters.\n",
    "print(cl.get_optimal_hyperparameters(exp_name))"
   ],
   "id": "5a519aa3bcd8ce5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thank you for using `tune` API for LLM hyperparameter optimization. This feature is in the alpha stage. Kubeflow community is looking for your feedback. Please share your experience via #kubeflow-katib Slack channel or the Kubeflow Katib GitHub.\n",
      "PVC 'testllm' already exists in namespace kubeflow.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Object of type LoraRuntimeConfig is not JSON serializable",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[58], line 3\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# Fine-tuning for Binary Classification\u001B[39;00m\n\u001B[1;32m      2\u001B[0m exp_name \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtestllm\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m----> 3\u001B[0m \u001B[43mcl\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtune\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m      4\u001B[0m \u001B[43m    \u001B[49m\u001B[43mname\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mexp_name\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m      5\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmodel_provider_parameters\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mhf_model\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m      6\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdataset_provider_parameters\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mhf_dataset\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m      7\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtrainer_parameters\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mhf_tuning_parameters\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m      8\u001B[0m \u001B[43m    \u001B[49m\u001B[43mobjective_metric_name\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mtrain_loss\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m      9\u001B[0m \u001B[43m    \u001B[49m\u001B[43mobjective_type\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mminimize\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m     10\u001B[0m \u001B[43m    \u001B[49m\u001B[43malgorithm_name\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mrandom\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m     11\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;66;43;03m# max_trial_count = 10,\u001B[39;49;00m\n\u001B[1;32m     12\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;66;43;03m# parallel_trial_count = 2,\u001B[39;49;00m\n\u001B[1;32m     13\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;66;43;03m# resources_per_trial={\u001B[39;49;00m\n\u001B[1;32m     14\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;66;43;03m#     \"gpu\": \"2\",\u001B[39;49;00m\n\u001B[1;32m     15\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;66;43;03m#     \"cpu\": \"4\",\u001B[39;49;00m\n\u001B[1;32m     16\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;66;43;03m#     \"memory\": \"10G\",\u001B[39;49;00m\n\u001B[1;32m     17\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;66;43;03m# },\u001B[39;49;00m\n\u001B[1;32m     18\u001B[0m \u001B[43m)\u001B[49m\n\u001B[1;32m     20\u001B[0m cl\u001B[38;5;241m.\u001B[39mwait_for_experiment_condition(name\u001B[38;5;241m=\u001B[39mexp_name)\n\u001B[1;32m     22\u001B[0m \u001B[38;5;66;03m# Get the best hyperparameters.\u001B[39;00m\n",
      "File \u001B[0;32m~/miniconda3/envs/llm-hp-optimization-katib-nb/lib/python3.9/site-packages/kubeflow/katib/api/katib_client.py:605\u001B[0m, in \u001B[0;36mKatibClient.tune\u001B[0;34m(self, name, model_provider_parameters, dataset_provider_parameters, trainer_parameters, storage_config, objective, base_image, parameters, namespace, env_per_trial, algorithm_name, algorithm_settings, objective_metric_name, additional_metric_names, objective_type, objective_goal, max_trial_count, parallel_trial_count, max_failed_trial_count, resources_per_trial, retain_trials, packages_to_install, pip_index_url, metrics_collector_config)\u001B[0m\n\u001B[1;32m    601\u001B[0m trial_params \u001B[38;5;241m=\u001B[39m []\n\u001B[1;32m    602\u001B[0m training_args \u001B[38;5;241m=\u001B[39m utils\u001B[38;5;241m.\u001B[39mget_trial_substitutions_from_trainer(\n\u001B[1;32m    603\u001B[0m     trainer_parameters\u001B[38;5;241m.\u001B[39mtraining_parameters, experiment_params, trial_params\n\u001B[1;32m    604\u001B[0m )\n\u001B[0;32m--> 605\u001B[0m lora_config \u001B[38;5;241m=\u001B[39m \u001B[43mutils\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_trial_substitutions_from_trainer\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    606\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtrainer_parameters\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlora_config\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mexperiment_params\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrial_params\u001B[49m\n\u001B[1;32m    607\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    609\u001B[0m \u001B[38;5;66;03m# Create the init and the primary container.\u001B[39;00m\n\u001B[1;32m    610\u001B[0m init_container_spec \u001B[38;5;241m=\u001B[39m training_utils\u001B[38;5;241m.\u001B[39mget_container_spec(\n\u001B[1;32m    611\u001B[0m     name\u001B[38;5;241m=\u001B[39mSTORAGE_INITIALIZER,\n\u001B[1;32m    612\u001B[0m     base_image\u001B[38;5;241m=\u001B[39mSTORAGE_INITIALIZER_IMAGE,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    625\u001B[0m     volume_mounts\u001B[38;5;241m=\u001B[39m[STORAGE_INITIALIZER_VOLUME_MOUNT],\n\u001B[1;32m    626\u001B[0m )\n",
      "File \u001B[0;32m~/miniconda3/envs/llm-hp-optimization-katib-nb/lib/python3.9/site-packages/kubeflow/katib/utils/utils.py:213\u001B[0m, in \u001B[0;36mget_trial_substitutions_from_trainer\u001B[0;34m(parameters, experiment_params, trial_params)\u001B[0m\n\u001B[1;32m    211\u001B[0m     parameters \u001B[38;5;241m=\u001B[39m json\u001B[38;5;241m.\u001B[39mdumps(parameters\u001B[38;5;241m.\u001B[39mto_dict())\n\u001B[1;32m    212\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 213\u001B[0m     parameters \u001B[38;5;241m=\u001B[39m \u001B[43mjson\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdumps\u001B[49m\u001B[43m(\u001B[49m\u001B[43mparameters\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[38;5;18;43m__dict__\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mcls\u001B[39;49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mSetEncoder\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    215\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m parameters\n",
      "File \u001B[0;32m~/miniconda3/envs/llm-hp-optimization-katib-nb/lib/python3.9/json/__init__.py:234\u001B[0m, in \u001B[0;36mdumps\u001B[0;34m(obj, skipkeys, ensure_ascii, check_circular, allow_nan, cls, indent, separators, default, sort_keys, **kw)\u001B[0m\n\u001B[1;32m    232\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mcls\u001B[39m \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    233\u001B[0m     \u001B[38;5;28mcls\u001B[39m \u001B[38;5;241m=\u001B[39m JSONEncoder\n\u001B[0;32m--> 234\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mcls\u001B[39;49m\u001B[43m(\u001B[49m\n\u001B[1;32m    235\u001B[0m \u001B[43m    \u001B[49m\u001B[43mskipkeys\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mskipkeys\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mensure_ascii\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mensure_ascii\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    236\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcheck_circular\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcheck_circular\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mallow_nan\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mallow_nan\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mindent\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mindent\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    237\u001B[0m \u001B[43m    \u001B[49m\u001B[43mseparators\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mseparators\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdefault\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdefault\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msort_keys\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msort_keys\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    238\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkw\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mencode\u001B[49m\u001B[43m(\u001B[49m\u001B[43mobj\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/envs/llm-hp-optimization-katib-nb/lib/python3.9/json/encoder.py:199\u001B[0m, in \u001B[0;36mJSONEncoder.encode\u001B[0;34m(self, o)\u001B[0m\n\u001B[1;32m    195\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m encode_basestring(o)\n\u001B[1;32m    196\u001B[0m \u001B[38;5;66;03m# This doesn't pass the iterator directly to ''.join() because the\u001B[39;00m\n\u001B[1;32m    197\u001B[0m \u001B[38;5;66;03m# exceptions aren't as detailed.  The list call should be roughly\u001B[39;00m\n\u001B[1;32m    198\u001B[0m \u001B[38;5;66;03m# equivalent to the PySequence_Fast that ''.join() would do.\u001B[39;00m\n\u001B[0;32m--> 199\u001B[0m chunks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43miterencode\u001B[49m\u001B[43m(\u001B[49m\u001B[43mo\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m_one_shot\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[1;32m    200\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(chunks, (\u001B[38;5;28mlist\u001B[39m, \u001B[38;5;28mtuple\u001B[39m)):\n\u001B[1;32m    201\u001B[0m     chunks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlist\u001B[39m(chunks)\n",
      "File \u001B[0;32m~/miniconda3/envs/llm-hp-optimization-katib-nb/lib/python3.9/json/encoder.py:257\u001B[0m, in \u001B[0;36mJSONEncoder.iterencode\u001B[0;34m(self, o, _one_shot)\u001B[0m\n\u001B[1;32m    252\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    253\u001B[0m     _iterencode \u001B[38;5;241m=\u001B[39m _make_iterencode(\n\u001B[1;32m    254\u001B[0m         markers, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdefault, _encoder, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mindent, floatstr,\n\u001B[1;32m    255\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mkey_separator, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mitem_separator, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msort_keys,\n\u001B[1;32m    256\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mskipkeys, _one_shot)\n\u001B[0;32m--> 257\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_iterencode\u001B[49m\u001B[43m(\u001B[49m\u001B[43mo\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/envs/llm-hp-optimization-katib-nb/lib/python3.9/site-packages/kubeflow/katib/utils/utils.py:143\u001B[0m, in \u001B[0;36mSetEncoder.default\u001B[0;34m(self, obj)\u001B[0m\n\u001B[1;32m    141\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(obj, \u001B[38;5;28mtype\u001B[39m):\n\u001B[1;32m    142\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m obj\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m\n\u001B[0;32m--> 143\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mjson\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mJSONEncoder\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdefault\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mobj\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/envs/llm-hp-optimization-katib-nb/lib/python3.9/json/encoder.py:179\u001B[0m, in \u001B[0;36mJSONEncoder.default\u001B[0;34m(self, o)\u001B[0m\n\u001B[1;32m    160\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdefault\u001B[39m(\u001B[38;5;28mself\u001B[39m, o):\n\u001B[1;32m    161\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Implement this method in a subclass such that it returns\u001B[39;00m\n\u001B[1;32m    162\u001B[0m \u001B[38;5;124;03m    a serializable object for ``o``, or calls the base implementation\u001B[39;00m\n\u001B[1;32m    163\u001B[0m \u001B[38;5;124;03m    (to raise a ``TypeError``).\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    177\u001B[0m \n\u001B[1;32m    178\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m--> 179\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mObject of type \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mo\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m \u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[1;32m    180\u001B[0m                     \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mis not JSON serializable\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "\u001B[0;31mTypeError\u001B[0m: Object of type LoraRuntimeConfig is not JSON serializable"
     ]
    }
   ],
   "execution_count": 58
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "ec2220c3bc4896af"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

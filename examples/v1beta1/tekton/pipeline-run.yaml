---
# This example shows how you can use Tekton Pipelines in Katib, transfer parameters from one Task to another and run HP job.
# It uses simple random algorithm and tunes only learning rate.
# Pipelines contains 2 Tasks, first is data-preprocessing second is model-training.
# First Task shows how you can prepare your training data (here: simply divide number of training examples) before running HP job.
# Number of training examples is transferred to the second Task.
# Second Task is the actual training which metrics collector sidecar is injected.
# Note that for this example Tekton controller's nop image must be equal to StdOut metrics collector image.
apiVersion: kubeflow.org/v1beta1
kind: Experiment
metadata:
  namespace: kubeflow
  name: tekton-pipeline-run
spec:
  objective:
    type: minimize
    goal: 0.001
    objectiveMetricName: loss
  algorithm:
    algorithmName: random
  parallelTrialCount: 2
  maxTrialCount: 4
  maxFailedTrialCount: 3
  parameters:
    - name: lr
      parameterType: double
      feasibleSpace:
        min: "0.01"
        max: "0.03"
  trialTemplate:
    retain: true
    primaryPodLabels:
      tekton.dev/pipelineTask: model-training
    primaryContainerName: step-model-training
    successCondition: status.conditions.#(type=="Succeeded")#|#(status=="True")#
    failureCondition: status.conditions.#(type=="Succeeded")#|#(status=="False")#
    trialParameters:
      - name: learningRate
        description: Learning rate for the training model
        reference: lr
    trialSpec:
      apiVersion: tekton.dev/v1beta1
      kind: PipelineRun
      spec:
        params:
          - name: learningRate
            description: Learning rate for the training model
            reference: lr
          - name: epochs-init
            value: "60000"
        pipelineSpec:
          params:
            - name: lr
              description: Learning rate for the training model
            - name: epochs-init
              description: Initial value for number of training examples
          tasks:
            - name: data-preprocessing
              params:
                - name: epochs-pre
                  value: $(params.epochs-init)
              taskSpec:
                params:
                  - name: epochs-pre
                    description: Number of training examples before optimization
                results:
                  - name: epochs-post
                    description: Number of training examples after optimization
                steps:
                  - name: epochs-optimize
                    image: python:alpine3.6
                    command:
                      - sh
                      - -c
                    args:
                      - python3 -c "import random; print($(params.epochs-pre)//random.randint(3000,30000),end='')" | tee $(results.epochs-post.path)
            - name: model-training
              params:
                - name: lr
                  value: $(params.lr)
                - name: epochs
                  value: $(tasks.data-preprocessing.results.epochs-post)
              taskSpec:
                params:
                  - name: lr
                    description: Learning rate for the training model
                  - name: epochs
                    description: Number of epochs
                steps:
                  - name: model-training
                    image: ghcr.io/kubeflow/katib/pytorch-mnist-cpu:latest
                    command:
                      - "python3"
                      - "/opt/pytorch-mnist/mnist.py"
                      - "--epochs=$(params.epochs)"
                      - "--batch-size=16"
                      - "--lr=$(trialParameters.lr)"
